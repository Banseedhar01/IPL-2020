# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e_hhZwKQWfi9IQfz3LNosOQTTvYCVvFy
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt

from sklearn.datasets import load_boston

boston = load_boston()

print(boston.DESCR)

house_data = pd.DataFrame(boston.data)
house_data.head()

house_data.columns = boston.feature_names
house_data['price'] = boston.target
house_data.head()

df = pd.DataFrame(house_data[['RM', 'price']])

df.head()

df.shape

x = df['RM']
y = df['price']

x = (x - x.mean())/x.std()
x = np.c_[np.ones(x.shape[0]), x]

x.shape

def loss(h, y):
  sq_error = (h - y) ** 2
  n = len(y)
  return (1/2*n)*sq_error.sum()

class LinearRegression:
  def predict(self, X):
    return np.dot(X, self._W)

  def _gradient_descent_step(self, X, targets, lr):

    predictions = self.predict(X)

    error = predictions - targets
    gradient = np.dot(X.T, error)/len(X)
    
    self._W -= lr * gradient

  def fit(self, X, y, n_iter=10000, lr=0.01):

    self._W = np.zeros(X.shape[1])

    self.cost_history = []
    self.w_history = [self._W]

    for i in range(n_iter):

      predictions = self.predict(X)
      cost = loss(predictions, y)

      self.cost_history.append(cost)
      self._gradient_descent_step(X, y, lr)
      self.w_history.append(self._W.copy())

    return self

linear_model = LinearRegression()
linear_model.fit(x, y, n_iter=2000, lr=0.01)

linear_model._W

plt.title('Cost Function J')
plt.xlabel('No. of iterations')
plt.ylabel('Cost')
plt.plot(linear_model.cost_history)
plt.show()

linear_model.cost_history
#with sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
model = LinearRegression()
model.fit(x_train, y_train)